Symbolic Unconditional Music Generation
=====

## Introduction: Train a model and use that model to generate Lo-fi music 
This project implements an AI-powered Lo-Fi music generator using Long Short-Term Memory (LSTM) neural networks. By analyzing patterns in a dataset of Lo-fi MIDI files, the system learns to create original, soothing musical compositions characteristic of the popular Lo-Fi hip-hop genre.


## Model: Long Short Term Memory (LSTM)

### What is a LSTM? 
`"Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models, and other sequence learning methods."` (From Wikipedia).  
LSTMs are explicitly designed to handle **long term dependency problems**. Remebering information for a long period of time is practically their defualt behavior. The core innovation of LSTMs is their ability to selectively learn what information is important to **remember over long sequences** and what can be safely forgotten. It is this ability that made us think that LSTM can out perform the traditional Markov Chains. We have generated files with both models for comparison.

## Training Data
- CSE 153R SP25, Midi file data from Assignment 1, Task 1. Located in the Assignment1(Task1_mdidis) folder.
- From Kaggle(Lofi-Hip-Hop), this dataset is organised and published by Zachary Katsnelson. You can find the midi files in the Lo-Fi folder or you can read/download it from: https://www.kaggle.com/datasets/zakarii/lofi-hip-hop-midi?resource=download (Last accessed May 25, 2025).
   - We are focusing on just one instrument: Grand Piano, which is very commonly used in Lo-fi music. This whole training data uses only Grand Piano, which is embedded as program 0 in MIDI files.
   - This dataset was create for a similar purpose: to be trained on and generate Lo-fi music. The composition of this dataset is strongly influenced by Nujabes and J Dilla, both very famous Lo-fi music creators.
   - This dataset has also been used previously for VAE, audio analysis, RNNs, and some other Lo-fi music generation projects. See dtails about them in https://www.kaggle.com/datasets/zakarii/lofi-hip-hop-midi/code
- In the data folder, is a bunch of popular pop songs I personally enjoy. And was able to gather up some of it in midi form.

## Method

The Jupyter notebook has a very detailed exploration of the whole process, here is a brief outline:

1. Train a tokenizer with REMI method.
   - For simplicity, we are ignoring how hard notes are pressed and treating chords as single items.
   - In addition, since we know all data are all grand piano, we can ignore the program (instrument)

2. Load the training data and extract all the notes using the tokenizer.

3. Define the architecture and build the LSTM model

4. Train the model
    - Adam optimizer
    - 100 epochs

6. Generate music using the model
    - Option one: use grand piano as the single instrument since it is indeed what we are trained on.
    - Option two: create a multi-instrument midi by creating seperate tracks for each instrument then combining them
    - Option three: assign random Lo-fi-related instruments to to each of the generated notes

## Evaluations

We are comparing music generated by LSTM to music generated by Markov Chains (Unigram and Bigram) along with randomly generated music. Since Markov Chains and random music have nothing to optimize, the best we can do is manually compare the music they generated and decide which is better. The baseline, of course, would be randomly generated music and we are aiming to produce music better than the ones generated by Markov Chains, which we used previously and decided that there are still room for improvement.


## Future Expansions

The final result produced by LSTM was satisfying, but there are still room for improvement

1. For this project, we are ignoring how hard the notes are pressed and the gap between each note is constant. For future expansion, we can modify or use a new tokenizer to capture these feature and train the LSTM to learn about them.
2. This project focuses on just the grand piano, but there are some more instruments that Lo-fi music generally use, such as guitars, synthesizers, and drum. For future expansion, we can find datasets that contain more instruments and use LSTM to learn their patterns.
3. Find someone with deeper knowledge of music theory to help us evaluate our generated music.


## Conclusion
By blending in machine learning during music creation, this system demonstrates how AI can creatively participate in music production while honoring the distinctive aesthetic qualities of the Lo-Fi genre. The generated pieces feature the calming, nostalgic qualities that make Lo-Fi a beloved genre for relaxation and focus.
