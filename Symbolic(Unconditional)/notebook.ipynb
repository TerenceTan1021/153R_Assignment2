{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Installations Required for Generating Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from numpy.random import choice\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from midiutil import MIDIFile\n",
    "import mido\n",
    "from midi_utils import midi_to_note_duration_sequence, note_duration_sequence_to_midi\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods for sequence extraction:\n",
    " - `midi_to_note_duration_sequence()`: Extract (pitch, duration) tuples from a MIDI file.\n",
    " - `note_sequence_to_midi()`: Create a MIDI file from a sequence of notes, assigning a fixed duration (note_length) to all notes.\n",
    " - `note_duration_sequence_to_midi()` : Generate a MIDI file with custom durations for each note, allowing variable-length notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_note_duration_sequence(filename):\n",
    "    mid = mido.MidiFile(filename)\n",
    "    notes = []\n",
    "    abs_time = 0\n",
    "    note_on_times = {}\n",
    "    for msg in mid:\n",
    "        abs_time += msg.time\n",
    "        if msg.type == 'note_on' and msg.velocity > 0:\n",
    "            note_on_times[msg.note] = abs_time\n",
    "        elif (msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0)) and msg.note in note_on_times:\n",
    "            duration = abs_time - note_on_times[msg.note]\n",
    "            notes.append((msg.note, int(duration * 480)))  # scale duration for MIDI ticks\n",
    "            del note_on_times[msg.note]\n",
    "    return notes\n",
    "\n",
    "def note_sequence_to_midi(note_sequence, filename, velocity=64, tempo=500000, note_length=120):\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=tempo))\n",
    "    for note in note_sequence:\n",
    "        track.append(mido.Message('note_on', note=note, velocity=velocity, time=0))\n",
    "        track.append(mido.Message('note_off', note=note, velocity=velocity, time=note_length))\n",
    "    mid.save(filename)\n",
    "\n",
    "def note_duration_sequence_to_midi(note_duration_sequence, filename, velocity=64, tempo=500000):\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "    track.append(mido.MetaMessage('set_tempo', tempo=tempo))\n",
    "    for note, duration in note_duration_sequence:\n",
    "        track.append(mido.Message('note_on', note=note, velocity=velocity, time=0))\n",
    "        track.append(mido.Message('note_off', note=note, velocity=velocity, time=duration))\n",
    "    mid.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Parameter Setup, defining constraints for quantization and model input/output:\n",
    " - `seq_length`: Context window for the LSTM.\n",
    "\n",
    " - `possible_durations`: Quantizes note durations to these values.\n",
    "\n",
    " - `min_pitch`/`max_pitch`: Restricts pitches to the piano range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "possible_durations = [120, 240, 360, 480]\n",
    "min_pitch = 21\n",
    "max_pitch = 108\n",
    "\n",
    "# Random seed can be changed to get different results\n",
    "# default is 42, from CSE_153R Homework 3(Spring 2025)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Encoding/Decoding Logic. Maps (pitch, duration) pairs to/from a single integer for model compatibility:\n",
    " - `encode_pair()`: Combines pitch and duration into a unique index.\n",
    "\n",
    " - `decode_pair()`: Reverses the encoding to recover original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pair(note, duration):\n",
    "    return (note - min_pitch) * len(possible_durations) + possible_durations.index(duration)\n",
    "\n",
    "def decode_pair(idx):\n",
    "    pitch = min_pitch + idx // len(possible_durations)\n",
    "    duration = possible_durations[idx % len(possible_durations)]\n",
    "    return pitch, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Load the music data, extract the sequences, and encode the sequences\n",
    " - Quantizes raw durations to the nearest value in `possible_durations`.\n",
    "\n",
    " - Encodes all notes into a flat list of integers for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first is popular pop songs I enjoy\n",
    "midi_files = glob.glob('data/*.mid')\n",
    "# same midi file data We have trained on for Assigment 1 Task 1\n",
    "#midi_files = glob('Assignment1(Task1_midis)/*.midi')\n",
    "len(midi_files)\n",
    "\n",
    "sequences = [midi_to_note_duration_sequence(f) for f in midi_files]\n",
    "encoded = [encode_pair(note, min(possible_durations, key=lambda x: abs(x-duration))) for seq in sequences for note, duration in seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Sequence Preparation\n",
    " - Create input-output pairs using a sliding window of `seq_length (20)`.\n",
    " - Example: If `encoded` = `[0, 1, 2, 3, 4]`, then `X = [[0,1,2], [1,2,3]]`, `y = [3, 4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for i in range(len(encoded) - seq_length):\n",
    "    X.append(encoded[i:i+seq_length])\n",
    "    y.append(encoded[i+seq_length])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Architecture\n",
    " - Embedding: Converts encoded indices to dense vectors (dimensionality reduction).\n",
    "\n",
    " - LSTM: Learns temporal patterns in sequences.\n",
    "\n",
    " - Dense + Softmax: Predicts the probability distribution over possible pitch-duration pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = (max_pitch - min_pitch + 1) * len(possible_durations)\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=seq_length),\n",
    "    layers.LSTM(128, return_sequences=False),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Configuration\n",
    " - Uses `sparse_categorical_crossentropy` since labels are integers (not one-hot).\n",
    "\n",
    " - Trains for 20 epochs with Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 4.1903\n",
      "Epoch 2/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 2.9087\n",
      "Epoch 3/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 2.3554\n",
      "Epoch 4/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 2.0626\n",
      "Epoch 5/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 1.8522\n",
      "Epoch 6/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 1.6766\n",
      "Epoch 7/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 1.5174\n",
      "Epoch 8/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 1.3693\n",
      "Epoch 9/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 1.2369\n",
      "Epoch 10/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 1.1335\n",
      "Epoch 11/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.0407\n",
      "Epoch 12/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.9484\n",
      "Epoch 13/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.8690\n",
      "Epoch 14/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.7951\n",
      "Epoch 15/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.7313\n",
      "Epoch 16/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.6983\n",
      "Epoch 17/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.6239\n",
      "Epoch 18/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.5795\n",
      "Epoch 19/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.5414\n",
      "Epoch 20/20\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.4784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c9e2e3c1a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Music Generation Loop\n",
    " - Autoregressive generation: Uses the last `seq_length` notes to predict the next note.\n",
    "\n",
    " - Greedy sampling (`argmax`) selects the most probable note at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_idx = random.randint(0, len(X)-1)\n",
    "seed = X[seed_idx]\n",
    "generated = list(seed)\n",
    "for _ in range(256):\n",
    "    input_seq = np.array(generated[-seq_length:]).reshape(1, seq_length)\n",
    "    pred = np.argmax(model.predict(input_seq, verbose=0))\n",
    "    generated.append(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Decode and Convert into MIDI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence saved as lstm_generated.mid\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = [decode_pair(idx) for idx in generated]\n",
    "note_duration_sequence_to_midi(decoded_sequence, 'lstm_generated.mid')\n",
    "print(\"Generated sequence saved as lstm_generated.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
